commit 70f9d0d0ac7cd8c54b194e0d273ad4be90da63a7	70f9d0d0ac7cd8c54b194e0d273ad4be90da63a7
Author: Jakub Łopuszański <jakub.lopuszanski@oracle.com>
Date:   Fri Apr 22 10:56:38 2022 +0200

    Bug #34097862 Innodb crashes when there is tremendous pressue on locking
    
    When transactions create a lot of record lock requests, timeout often,
    try to release waiting lock requests during semi-consistent read or
    otherwise cause a lot of pressure on lock-sys latches, this can lead to
    triggering sync array's long semaphore wait detection and consequently a
    crash, as InnoDB is unable to rule out the possibility that such long
    wait is caused by a deadlock.
    
    This patch tries to alieviate the congestion by optimizing several
    places which previously required exclusive global latch, so that now
    they can latch just the single shard they need to access.
    
    To make it possible, lock_cancel_waiting_and_release() was modified to
    no longer call lock_release_autoinc_locks(trx). I believe this call was
    added unnecessarily during fixing Bug #26316 "Triggers create duplicate
    entries on auto-increment columns". The auto-increment locks are
    released at the end of each query anyway, so it's not clear why bother
    releasing them manually from this particular function, even more so why
    do we do that only if the type of the lock the trx awaited was a table
    lock. More importantly, the testcase from that bug report passes without
    this particular call. The call was problematic as we could not predict
    which shard(s) will contain the auto increment locks, thus exclusive
    global latch was taken before each call to
    lock_cancel_waiting_and_realease(). Once the call is gone, we can focus
    on latching just the shard containing the trx->lock.wait_lock.
    
    Latching just a single shard, not the whole lock-sys during
    lock_cancel_waiting_and_release() introduced a slight complication in
    the proof of deadlock-freedom of the interaction between lock-sys and
    trx-sys: when performing lock_cancel_waiting_and_release(trx) we need to
    latch not only the trx, but potentially also heir_trx which will be
    granted the lock blocked by trx's lock (even a waiting lock request can
    block another lock request, thus releasing it can lead to granting).
    Thus this patch fixes the proof, by changing the rule which must be
    followed when latching two trxs in order: the first transaction might be
    waiting for a lock, but only if the thread is holding the latch for the
    shard which contains its wait_lock. Asserts were adjusted accordingly.
    
    Another ingridient is the introduction of run_if_waiting(trx,f) template
    which calls f() after latching the shard containing trx->lock.wait_lock,
    taking care of double checking that trx->lock.wait_lock has not changed
    while it was busy acquiring the latch - automatically retrying in case a
    record, and thus the record lock, were moved from one page to another
    during B-tree reorganization. While rare, this needs to be handled with
    care, as trx->lock.wait_lock can change if we don't latch the shard
    which contains it leading to a chicken-and-egg problem.
    
    In practice run_if_waiting takes as an input not just trx, but also its
    expected version number.
    
    With above two improvements, the following places which used exclusive
    global latch were optimized:
    
    1. innobase_kill_connection(thd) used to call lock_trx_handle_wait(trx)
       which canceled the trx->lock.wait_lock request if any, requiring
       exclusive global latch. It now uses run_if_waiting.
    
    2. lock_make_trx_hit_list(hp_trx,hit_list) used to traverse the lock
       queue containing hp_trx's waiting lock to populate the hit_list of
       transactions which block hp_trx and thus need to be forced to
       rollback. This function used to call
       lock_cancel_waiting_and_release(victim) while iterating, which
       required exclusive latch for several reasons listed in its
       documentation. Now, it will only add victim to the hit_list, which
       can be done while latching just the shard containing the lock queue,
       which can be accomplished with run_if_waiting(hp_trx,...).
       The call to lock_cancel_waiting_and_release(victim) now happens in a
       separate loop over hit_list, which requires latching only the
       victim's wait_lock's queue, made easy by run_if_waiting(victim,...).
    
    3. lock_wait_check_and_cancel(slot) needed exclusive global latch to
       call lock_cancel_waiting_and_release(..). Now it uses run_if_waiting.
    
    4. row_search_mvcc(...) used lock_trx_handle_wait(trx) to cancel waiting
       lock request in case of semi-consistent read in READ COMMITTED, as in
       this mode it prefers to check if the commited (older) version of the
       record matches the WHERE clause in the first place, before wasting
       time on waiting. This got simplified by using SELECT_SKIP_LOCKED mode
       to avoid creating a wating lock in the first place, so there's
       nothing to relase.
    
    Change-Id: Ic86beff12fbf05b2d9758c6cd3ec3e28b263b222

diff --git a/mysql-test/suite/innodb/r/lock_end_of_range.result b/mysql-test/suite/innodb/r/lock_end_of_range.result
index 0cba8c00a82..04a034eae54 100644
--- a/mysql-test/suite/innodb/r/lock_end_of_range.result
+++ b/mysql-test/suite/innodb/r/lock_end_of_range.result
@@ -8652,7 +8652,7 @@ BEGIN;
 SET DEBUG_SYNC='
     semi_consistent_read_would_wait
     SIGNAL con2_would_wait
-    WAIT_FOR con2_can_unlock';
+    WAIT_FOR con2_can_peek';
 SET DEBUG_SYNC='
     row_search_for_mysql_before_return
     SIGNAL con2_returns_row
@@ -8687,8 +8687,7 @@ PRIMARY	RECORD	X,REC_NOT_GAP	GRANTED	3
 PRIMARY	RECORD	X,REC_NOT_GAP	GRANTED	4
 PRIMARY	RECORD	X,REC_NOT_GAP	GRANTED	5
 PRIMARY	RECORD	X,REC_NOT_GAP	GRANTED	6
-PRIMARY	RECORD	X,REC_NOT_GAP	WAITING	6
-SET DEBUG_SYNC='now SIGNAL con2_can_unlock';
+SET DEBUG_SYNC='now SIGNAL con2_can_peek';
 SET DEBUG_SYNC='now WAIT_FOR con2_returns_row';
 SELECT index_name, lock_type, lock_mode, lock_status, lock_data
 FROM performance_schema.data_locks
diff --git a/mysql-test/suite/innodb/t/lock_end_of_range.test b/mysql-test/suite/innodb/t/lock_end_of_range.test
index 8ca8b1cd93a..dbfab980f29 100644
--- a/mysql-test/suite/innodb/t/lock_end_of_range.test
+++ b/mysql-test/suite/innodb/t/lock_end_of_range.test
@@ -530,8 +530,8 @@ DROP TABLE t1;
 # In this scenario, we test what happens when semi-consistent read is involved.
 # In particular con1 locks row with id=6 while deleting it, and con2 tries
 # to lock rows with id<=6 while updating them using semi-consistent read.
-# This scenario is interesting, because when con2 goes notices it would have to
-# wait, it cancels it's own waiting lock, and reports the old (non-deleted)
+# This scenario is interesting, because when con2 notices it would have to
+# wait, it avoids creating a waiting lock, and reports the old (non-deleted)
 # version of the row to the higher layer, which then retries the read, this time
 # with proper locking enabled - so it is interesting what would happen if con1
 # have had commited the delete meanwhile.
@@ -557,7 +557,7 @@ INSERT INTO t1 (id,val) VALUES (1,1),(2,2) ,(3,3), (4,4), (5,5), (6,6),   (9,9);
   SET DEBUG_SYNC='
     semi_consistent_read_would_wait
     SIGNAL con2_would_wait
-    WAIT_FOR con2_can_unlock';
+    WAIT_FOR con2_can_peek';
   SET DEBUG_SYNC='
     row_search_for_mysql_before_return
     SIGNAL con2_returns_row
@@ -581,7 +581,7 @@ INSERT INTO t1 (id,val) VALUES (1,1),(2,2) ,(3,3), (4,4), (5,5), (6,6),   (9,9);
   SELECT index_name, lock_type, lock_mode, lock_status, lock_data
     FROM performance_schema.data_locks
     WHERE object_name='t1';
-  SET DEBUG_SYNC='now SIGNAL con2_can_unlock';
+  SET DEBUG_SYNC='now SIGNAL con2_can_peek';
   SET DEBUG_SYNC='now WAIT_FOR con2_returns_row';
   --sorted_result
   SELECT index_name, lock_type, lock_mode, lock_status, lock_data
diff --git a/share/messages_to_error_log.txt b/share/messages_to_error_log.txt
index 311ddf95914..d6815b7b6ce 100644
--- a/share/messages_to_error_log.txt
+++ b/share/messages_to_error_log.txt
@@ -7440,7 +7440,7 @@ ER_IB_MSG_635
   eng "%s"
 
 ER_IB_MSG_636
-  eng "%s"
+  eng "Blocked High Priority Transaction (ID %llu, Thread ID %s) forces rollback of the blocking transaction (ID %llu - %s)."
 
 ER_IB_MSG_637
   eng "%s"
@@ -7449,7 +7449,7 @@ ER_IB_MSG_638
   eng "%s"
 
 ER_IB_MSG_639
-  eng "%s"
+  eng "Blocked High Priority Transaction (Thread ID %s) waking up the blocking transaction (ID %llu) by pretending it's a deadlock victim."
 
 OBSOLETE_ER_IB_MSG_640
   eng "%s"
@@ -9165,7 +9165,7 @@ ER_IB_MSG_1210
   eng "%s"
 
 ER_IB_MSG_1211
-  eng "%s"
+  eng "Blocked High Priority Transaction (ID %llu, Thread ID %s) killed the blocking transaction (ID %llu - %s) by rolling it back."
 
 ER_IB_MSG_1212
   eng "%s"
diff --git a/storage/innobase/handler/ha_innodb.cc b/storage/innobase/handler/ha_innodb.cc
index 629e323e462..5268d4ad3b1 100644
--- a/storage/innobase/handler/ha_innodb.cc
+++ b/storage/innobase/handler/ha_innodb.cc
@@ -6209,7 +6209,7 @@ static void innobase_kill_connection(
 
   if (trx != nullptr) {
     /* Cancel a pending lock request if there are any */
-    lock_trx_handle_wait(trx);
+    lock_cancel_if_waiting_and_release({trx});
   }
 }
 
diff --git a/storage/innobase/include/lock0guards.h b/storage/innobase/include/lock0guards.h
index 3e367d4abb3..c3663766394 100644
--- a/storage/innobase/include/lock0guards.h
+++ b/storage/innobase/include/lock0guards.h
@@ -93,7 +93,7 @@ class Shard_naked_latch_guard : private ut::Non_copyable {
 
  public:
   explicit Shard_naked_latch_guard(ut::Location location,
-                                   const dict_table_t &table);
+                                   const table_id_t &table_id);
 
   explicit Shard_naked_latch_guard(ut::Location location,
                                    const page_id_t &page_id);
@@ -118,7 +118,7 @@ class Shard_latch_guard {
  public:
   explicit Shard_latch_guard(ut::Location location, const dict_table_t &table)
       : m_global_shared_latch_guard{location},
-        m_shard_naked_latch_guard{location, table} {}
+        m_shard_naked_latch_guard{location, table.id} {}
 
   explicit Shard_latch_guard(ut::Location location, const page_id_t &page_id)
       : m_global_shared_latch_guard{location},
diff --git a/storage/innobase/include/lock0latches.h b/storage/innobase/include/lock0latches.h
index ca374084fd9..7184ac18904 100644
--- a/storage/innobase/include/lock0latches.h
+++ b/storage/innobase/include/lock0latches.h
@@ -26,6 +26,7 @@ this program; if not, write to the Free Software Foundation, Inc.,
 #ifndef lock0latches_h
 #define lock0latches_h
 
+#include "dict0types.h"
 #include "sync0sharded_rw.h"
 #include "ut0cpu_cache.h"
 #include "ut0mutex.h"
@@ -214,10 +215,10 @@ class Latches {
     Padded_mutex mutexes[SHARDS_COUNT];
     /**
     Identifies the table shard which contains locks for the given table.
-    @param[in]    table     The table
+    @param[in]    table_id    The id of the table
     @return Integer in the range [0..lock_sys_t::SHARDS_COUNT)
     */
-    static size_t get_shard(const dict_table_t &table);
+    static size_t get_shard(const table_id_t table_id);
 
    public:
     Table_shards();
@@ -225,14 +226,21 @@ class Latches {
 
     /** Returns the mutex which (together with the global latch) protects the
     table shard which contains table locks for the given table.
-    @param[in]    table     The table
+    @param[in]    table_id    The id of the table
     @return The mutex responsible for the shard containing the table
     */
-    Lock_mutex &get_mutex(const dict_table_t &table);
+    Lock_mutex &get_mutex(const table_id_t table_id);
 
     /** Returns the mutex which (together with the global latch) protects the
     table shard which contains table locks for the given table.
-    @param[in]    table     The table
+    @param[in]    table_id    The id of the table
+    @return The mutex responsible for the shard containing the table
+    */
+    const Lock_mutex &get_mutex(const table_id_t table_id) const;
+
+    /** Returns the mutex which (together with the global latch) protects the
+    table shard which contains table locks for the given table.
+    @param[in]    table    The table
     @return The mutex responsible for the shard containing the table
     */
     const Lock_mutex &get_mutex(const dict_table_t &table) const;
diff --git a/storage/innobase/include/lock0lock.h b/storage/innobase/include/lock0lock.h
index 9280a1a42d3..2661a4ae9f6 100644
--- a/storage/innobase/include/lock0lock.h
+++ b/storage/innobase/include/lock0lock.h
@@ -897,11 +897,27 @@ void lock_wait_suspend_thread(que_thr_t *thr); /*!< in: query thread associated
  function should be called at the the end of an SQL statement, by the
  connection thread that owns the transaction (trx->mysql_thd). */
 void lock_unlock_table_autoinc(trx_t *trx); /*!< in/out: transaction */
-/** Check whether the transaction has already been rolled back because it
- was selected as a deadlock victim, or if it has to wait then cancel
- the wait lock.
- @return DB_DEADLOCK, DB_LOCK_WAIT or DB_SUCCESS */
-dberr_t lock_trx_handle_wait(trx_t *trx); /*!< in/out: trx lock state */
+
+/** Cancels the waiting lock request of the trx, if any.
+If the transaction has already committed (trx->version has changed) or is no
+longer waiting for a lock (trx->lock.blocking_trx is nullptr) this function
+will not cancel the waiting lock.
+
+@note There is a possibility of ABA in which a waiting lock request was already
+granted or canceled and then the trx requested another lock and started waiting
+for it - in such case this function might either cancel or not the second
+request depending on timing. Currently all usages of this function ensure that
+this is impossible:
+- innodb_kill_connection ensures trx_is_interrupted(trx), thus upon first wake
+up it will realize it has to report an error and rollback
+- HP transaction marks the trx->in_innodb & TRX_FORCE_ROLLBACK flag which is
+checked when the trx attempts RecLock::add_to_waitq and reports DB_DEADLOCK
+
+@param[in]      trx_version   The trx we want to wake up and its expected
+                              version
+@return true iff the function did release a waiting lock
+*/
+bool lock_cancel_if_waiting_and_release(TrxVersion trx_version);
 
 /** Set the lock system timeout event. */
 void lock_set_timeout_event();
diff --git a/storage/innobase/include/lock0priv.h b/storage/innobase/include/lock0priv.h
index a3427fb8b94..4bce7b70f89 100644
--- a/storage/innobase/include/lock0priv.h
+++ b/storage/innobase/include/lock0priv.h
@@ -45,6 +45,7 @@ those functions in lock/ */
 #include "trx0types.h"
 #include "univ.i"
 
+#include <scope_guard.h>
 #include <utility>
 
 /** A table lock */
@@ -889,8 +890,8 @@ const lock_t *lock_rec_get_prev(
 
 /** Cancels a waiting lock request and releases possible other transactions
 waiting behind it.
-@param[in,out]  lock            Waiting lock request */
-void lock_cancel_waiting_and_release(lock_t *lock);
+@param[in,out]  trx    The transaction waiting for a lock */
+void lock_cancel_waiting_and_release(trx_t *trx);
 
 /** This function is a wrapper around several functions which need to be called
 in particular order to wake up a transaction waiting for a lock.
@@ -1100,6 +1101,112 @@ class Unsafe_global_latch_manipulator {
     lock_sys->latches.global_latch.x_lock(location);
   }
 };
+
+/** Temporarily releases trx->mutex, latches the lock-sys shard containing
+peeked_lock and latches trx->mutex again and calls f under protection of both
+latches. The latch on lock-sys shard will be released immediately after f
+returns. It is a responsibility of the caller to handle shared lock-sys latch,
+trx->mutex and verify inside f that the trx has not been finished, and the lock
+was not released meanwhile.
+@param[in]  peeked_lock   A lock of the trx. (While trx->mutex is held it can't
+                          be freed, but can be released). It is used to
+                          determine the lock-sys shard to latch.
+@param[in]  f             The callback to call once the lock-sys shard is
+                          latched and trx->mutex is relatched.
+@return The value returned by f.
+*/
+template <typename F>
+auto latch_peeked_shard_and_do(const lock_t *peeked_lock, F &&f) {
+  ut_ad(locksys::owns_shared_global_latch());
+  const trx_t *trx = peeked_lock->trx;
+  ut_ad(trx_mutex_own(trx));
+  ut_ad(peeked_lock->trx == trx);
+  /* peeked_wait_lock points to a lock struct which will not be freed while we
+  hold trx->mutex. Thus it is safe to inspect the peeked_wait_lock's
+  rec_lock.page_id and tab_lock.table. We have to make a copy of them, though,
+  before releasing trx->mutex. */
+  if (peeked_lock->is_record_lock()) {
+    const auto sharded_by = peeked_lock->rec_lock.page_id;
+    trx_mutex_exit(trx);
+    DEBUG_SYNC_C("try_relatch_trx_and_shard_and_do_noted_expected_version");
+    locksys::Shard_naked_latch_guard guard{UT_LOCATION_HERE, sharded_by};
+    trx_mutex_enter_first_of_two(trx);
+    return std::forward<F>(f)();
+  } else {
+    /*Once we release the trx->mutex, the trx may release locks on table and
+    commit, which in extreme case could lead to freeing the dict_table_t
+    object, so we have to copy its id first. */
+    const auto sharded_by = peeked_lock->tab_lock.table->id;
+    trx_mutex_exit(trx);
+    locksys::Shard_naked_latch_guard guard{UT_LOCATION_HERE, sharded_by};
+    trx_mutex_enter_first_of_two(trx);
+    return std::forward<F>(f)();
+  }
+}
+
+/** Given a pointer to trx (which the caller guarantees will not be freed) and
+the expected value of trx->version, will call the provided function f, only if
+the trx is still in expected version and waiting for a lock, within a critical
+section which holds latches on the trx, and the shard containing the waiting
+lock. If the transaction has meanwhile finished waiting for a lock, or committed
+or rolled back etc. the f will not be called.
+It may happen that the lock for which the trx is waiting during exectuion of f
+is not the same as the lock it was waiting at the moment of invocation.
+@param[in]  trx_version   The version of the trx that we intend to wake up
+@param[in]  f             The callback to call if trx is still waiting for a
+                          lock and is still in version trx_version
+*/
+template <typename F>
+void run_if_waiting(const TrxVersion trx_version, F &&f) {
+  const trx_t *trx = trx_version.m_trx;
+  /* This code would be much simpler with Global_exclusive_latch_guard.
+  Unfortunately, this lead to long semaphore waits when thousands of
+  transactions were taking thousands of locks and timing out. Therefore we use
+  the following tricky code to instead only latch the single shard which
+  contains the trx->lock.wait_lock. This is a bit difficult, because during
+  B-tree reorganization a record lock might be removed from one page and moved
+  to another, temporarily setting wait_lock to nullptr. This should be very
+  rare and short. In most cases this while loop should do just one iteration
+  and proceed along a happy path through all ifs. Another reason wait_lock
+  might become nullptr is because we were granted the lock meanwhile, in which
+  case the trx->lock.blocking_trx is first set to nullptr */
+  do {
+    if (!trx->lock.wait_lock.load()) {
+      continue;
+    }
+    locksys::Global_shared_latch_guard shared_latch_guard{UT_LOCATION_HERE};
+    /* We can't use IB_mutex_guard with trx->mutex, as trx_mutex_enter has
+    custom logic. We want to release trx->mutex before ut_delay or return. */
+    trx_mutex_enter(trx);
+    auto guard = create_scope_guard([trx]() { trx_mutex_exit(trx); });
+    if (trx->version != trx_version.m_version) {
+      return;
+    }
+    if (const lock_t *peeked_wait_lock = trx->lock.wait_lock.load()) {
+      const bool retry = latch_peeked_shard_and_do(peeked_wait_lock, [&]() {
+        ut_ad(trx_mutex_own(trx));
+        if (trx->version != trx_version.m_version) {
+          return false;
+        }
+        if (peeked_wait_lock != trx->lock.wait_lock.load()) {
+          /* If wait_lock has changed, then in case of record lock it might have
+          been moved during B-tree reorganization, so we retry. In case of a
+          table lock the wait_lock can not be "moved" so it had to be released
+          permanently and there's no point in retrying.*/
+          return peeked_wait_lock->is_record_lock();
+        }
+        std::forward<F>(f)();
+        ut_ad(trx_mutex_own(trx));
+        return false;
+      });
+      if (!retry) {
+        return;
+      }
+    }
+    /* wait_lock appears to be null. If blocking_trx isn't nullptr, then
+    probably the wait_lock will soon be restored, otherwise we can give up */
+  } while (trx->lock.blocking_trx.load() && ut_delay(10));
+}
 }  // namespace locksys
 
 #endif /* lock0priv_h */
diff --git a/storage/innobase/include/trx0trx.h b/storage/innobase/include/trx0trx.h
index c2b0c96e5aa..a281ef10eb9 100644
--- a/storage/innobase/include/trx0trx.h
+++ b/storage/innobase/include/trx0trx.h
@@ -1076,7 +1076,7 @@ struct trx_t {
   instance is re-used in trx_start_low(). It is used to track
   whether a transaction has been restarted since it was tagged
   for asynchronous rollback. */
-  ulint version;
+  std::atomic_uint64_t version;
 
   XID *xid;                    /*!< X/Open XA transaction
                                identification to identify a
@@ -1344,10 +1344,11 @@ struct commit_node_t {
   enum commit_node_state state; /*!< node execution state */
 };
 
+#ifdef UNIV_DEBUG
+
 /** Test if trx->mutex is owned by the current thread. */
-#define trx_mutex_own(t) mutex_own(&t->mutex)
+bool inline trx_mutex_own(const trx_t *trx) { return mutex_own(&trx->mutex); }
 
-#ifdef UNIV_DEBUG
 /**
 Verifies the invariants and records debug state related to latching rules.
 Called during trx_mutex_enter before the actual mutex acquisition.
diff --git a/storage/innobase/include/trx0types.h b/storage/innobase/include/trx0types.h
index 1f85a3fbec6..83684e8ada1 100644
--- a/storage/innobase/include/trx0types.h
+++ b/storage/innobase/include/trx0types.h
@@ -598,7 +598,7 @@ struct TrxVersion {
   TrxVersion(trx_t *trx);
 
   trx_t *m_trx;
-  ulint m_version;
+  uint64_t m_version;
 };
 
 typedef std::vector<TrxVersion, ut::allocator<TrxVersion>> hit_list_t;
diff --git a/storage/innobase/lock/lock0guards.cc b/storage/innobase/lock/lock0guards.cc
index 915f8242b74..b7300420755 100644
--- a/storage/innobase/lock/lock0guards.cc
+++ b/storage/innobase/lock/lock0guards.cc
@@ -67,9 +67,9 @@ Shard_naked_latch_guard::Shard_naked_latch_guard(ut::Location location,
 }
 
 Shard_naked_latch_guard::Shard_naked_latch_guard(ut::Location location,
-                                                 const dict_table_t &table)
+                                                 const table_id_t &table_id)
     : Shard_naked_latch_guard{
-          location, lock_sys->latches.table_shards.get_mutex(table)} {}
+          location, lock_sys->latches.table_shards.get_mutex(table_id)} {}
 
 Shard_naked_latch_guard::Shard_naked_latch_guard(ut::Location location,
                                                  const page_id_t &page_id)
diff --git a/storage/innobase/lock/lock0latches.cc b/storage/innobase/lock/lock0latches.cc
index 214a123cb10..dc65a4d8988 100644
--- a/storage/innobase/lock/lock0latches.cc
+++ b/storage/innobase/lock/lock0latches.cc
@@ -62,20 +62,25 @@ Lock_mutex &Latches::Page_shards::get_mutex(const page_id_t &page_id) {
       const_cast<const Latches::Page_shards *>(this)->get_mutex(page_id));
 }
 
-size_t Latches::Table_shards::get_shard(const dict_table_t &table) {
-  return table.id % SHARDS_COUNT;
+size_t Latches::Table_shards::get_shard(const table_id_t table_id) {
+  return table_id % SHARDS_COUNT;
 }
 
 const Lock_mutex &Latches::Table_shards::get_mutex(
-    const dict_table_t &table) const {
-  return mutexes[get_shard(table)];
+    const table_id_t table_id) const {
+  return mutexes[get_shard(table_id)];
 }
 
-Lock_mutex &Latches::Table_shards::get_mutex(const dict_table_t &table) {
+Lock_mutex &Latches::Table_shards::get_mutex(const table_id_t table_id) {
   /* See "Effective C++ item 3: Use const whenever possible" for explanation of
   this pattern, which avoids code duplication by reusing const version. */
   return const_cast<Lock_mutex &>(
-      const_cast<const Latches::Table_shards *>(this)->get_mutex(table));
+      const_cast<const Latches::Table_shards *>(this)->get_mutex(table_id));
+}
+
+const Lock_mutex &Latches::Table_shards::get_mutex(
+    const dict_table_t &table) const {
+  return get_mutex(table.id);
 }
 
 thread_local size_t Latches::Unique_sharded_rw_lock::m_shard_id{NOT_IN_USE};
diff --git a/storage/innobase/lock/lock0lock.cc b/storage/innobase/lock/lock0lock.cc
index b9e12b918ca..0039640a82d 100644
--- a/storage/innobase/lock/lock0lock.cc
+++ b/storage/innobase/lock/lock0lock.cc
@@ -49,6 +49,7 @@ this program; if not, write to the Free Software Foundation, Inc.,
 #include "ha_prototypes.h"
 #include "lock0lock.h"
 #include "lock0priv.h"
+#include "os0thread.h"
 #include "pars0pars.h"
 #include "row0mysql.h"
 #include "row0sel.h"
@@ -1407,10 +1408,9 @@ static void lock_mark_trx_for_rollback(hit_list_t &hit_list, trx_id_t hp_trx_id,
 
   if (thd != nullptr) {
     char buffer[1024];
-    ib::info(ER_IB_MSG_636)
-        << "Blocking transaction: ID: " << trx->id << " - "
-        << " Blocked transaction ID: " << hp_trx_id << " - "
-        << thd_security_context(thd, buffer, sizeof(buffer), 512);
+    ib::info(ER_IB_MSG_636, ulonglong{hp_trx_id}, to_string(thread_id).c_str(),
+             ulonglong{trx->id},
+             thd_security_context(thd, buffer, sizeof(buffer), 512));
   }
 #endif /* UNIV_DEBUG */
 }
@@ -2045,94 +2045,59 @@ void lock_make_trx_hit_list(trx_t *hp_trx, hit_list_t &hit_list) {
   const trx_id_t hp_trx_id = hp_trx->id;
   ut_ad(trx_can_be_handled_by_current_thread(hp_trx));
   ut_ad(trx_is_high_priority(hp_trx));
-  /* To avoid slow procedure involving global exclusive latch below, we first
+  /* To avoid slow procedure below, we first
   check if this transaction is waiting for a lock at all. It's unsafe to read
   hp->lock.wait_lock without latching whole lock_sys as it might temporarily
   change to NULL during a concurrent B-tree reorganization, even though the
-  trx actually is still waiting.
-  TBD: Is it safe to use hp_trx->lock.que_state == TRX_QUE_LOCK_WAIT given that
-  que_state is not atomic, and writes to it happen without trx->mutex ? */
+  trx actually is still waiting. Thus we use hp_trx->lock.blocking_trx instead.
+  */
   const bool is_waiting = (hp_trx->lock.blocking_trx.load() != nullptr);
   trx_mutex_exit(hp_trx);
   if (!is_waiting) {
     return;
   }
+  /* We don't expect hp_trx to commit (change version) as we are the thread
+  running the hp_trx */
+  locksys::run_if_waiting({hp_trx}, [&]() {
+    const lock_t *lock = hp_trx->lock.wait_lock;
+    // TBD: could this technique be used for table locks as well?
+    if (!lock->is_record_lock()) {
+      return;
+    }
+    trx_mutex_exit(hp_trx);
+    Lock_iter::for_each(
+        {lock, lock_rec_find_set_bit(lock)},
+        [&](lock_t *next) {
+          trx_t *trx = next->trx;
+          /* Check only for conflicting, granted locks on the current
+          row. Currently, we don't rollback read only transactions,
+          transactions owned by background threads. */
+          if (trx == hp_trx || next->is_waiting() || trx->read_only ||
+              trx->mysql_thd == nullptr || !lock_has_to_wait(lock, next)) {
+            return true;
+          }
 
-  /* Current implementation of lock_make_trx_hit_list requires latching whole
-  lock_sys for following reasons:
-  1. it may call lock_cancel_waiting_and_release on a lock from completely
-  different shard of lock_sys than hp_trx->lock.wait_lock. Trying to latch
-  this other shard might create a deadlock cycle if it violates ordering of
-  shard latches (and there is 50% chance it will violate it). Moreover the
-  lock_cancel_waiting_and_release() requires an exclusive latch to avoid
-  deadlocks among trx->mutex-es, and trx->lock.wait_lock might be a table lock,
-  in which case exclusive latch is also needed to traverse table locks.
-  2. it may call trx_mutex_enter on a transaction which is waiting for a
-  lock, which violates one of assumptions used in the proof that a deadlock due
-  to acquiring trx->mutex-es is impossible
-  3. it attempts to read hp_trx->lock.wait_lock which might be modified by a
-  thread during B-tree reorganization when moving locks between queues
-  4. it attempts to operate on trx->lock.wait_lock of other transactions */
-  locksys::Global_exclusive_latch_guard guard{UT_LOCATION_HERE};
-
-  /* Check again */
-  const lock_t *lock = hp_trx->lock.wait_lock;
-  if (lock == nullptr || !lock->is_record_lock()) {
-    return;
-  }
-  RecID rec_id{lock, lock_rec_find_set_bit(lock)};
-  Lock_iter::for_each(
-      rec_id,
-      [&](lock_t *next) {
-        trx_t *trx = next->trx;
-        /* Check only for conflicting, granted locks on the current
-        row. Currently, we don't rollback read only transactions,
-        transactions owned by background threads. */
-        if (trx == hp_trx || next->is_waiting() || trx->read_only ||
-            trx->mysql_thd == nullptr || !lock_has_to_wait(lock, next)) {
-          return true;
-        }
-
-        trx_mutex_enter(trx);
-
-        /* Skip high priority transactions, if already marked for
-        abort by some other transaction or if ASYNC rollback is
-        disabled. A transaction must complete kill/abort of a
-        victim transaction once marked and added to hit list. */
-        if (trx_is_high_priority(trx) ||
-            (trx->in_innodb & TRX_FORCE_ROLLBACK) != 0 ||
-            (trx->in_innodb & TRX_FORCE_ROLLBACK_DISABLE) != 0 || trx->abort) {
-          trx_mutex_exit(trx);
-
-          return true;
-        }
-
-        /* If the transaction is waiting on some other resource then
-        wake it up with DEAD_LOCK error so that it can rollback. */
-        if (trx->lock.que_state == TRX_QUE_LOCK_WAIT) {
-          /* Assert that it is not waiting for current record. */
-          ut_ad(trx->lock.wait_lock != next);
-#ifdef UNIV_DEBUG
-          ib::info(ER_IB_MSG_639)
-              << "High Priority Transaction (ID): " << lock->trx->id
-              << " waking up blocking"
-              << " transaction (ID): " << trx->id;
-#endif /* UNIV_DEBUG */
-          trx->lock.was_chosen_as_deadlock_victim = true;
-
-          lock_cancel_waiting_and_release(trx->lock.wait_lock);
+          trx_mutex_enter(trx);
+
+          /* Skip high priority transactions, if already marked for
+          abort by some other transaction or if ASYNC rollback is
+          disabled. A transaction must complete kill/abort of a
+          victim transaction once marked and added to hit list. */
+          if (!trx_is_high_priority(trx) &&
+              (trx->in_innodb & TRX_FORCE_ROLLBACK) == 0 &&
+              (trx->in_innodb & TRX_FORCE_ROLLBACK_DISABLE) == 0 &&
+              !trx->abort) {
+            /* Mark for ASYNC Rollback and add to hit list. */
+            lock_mark_trx_for_rollback(hit_list, hp_trx_id, trx);
+          }
 
           trx_mutex_exit(trx);
           return true;
-        }
-
-        /* Mark for ASYNC Rollback and add to hit list. */
-        lock_mark_trx_for_rollback(hit_list, hp_trx_id, trx);
-
-        trx_mutex_exit(trx);
-        return true;
-      },
-      lock->hash_table());
+        },
+        lock->hash_table());
+    // the run_if_waiting expects the hp_trx to be held after callback
+    trx_mutex_enter(hp_trx);
+  });
 }
 
 /** Cancels a waiting record lock request and releases the waiting transaction
@@ -4146,54 +4111,6 @@ static bool lock_release_read_lock(lock_t *lock, bool only_gap) {
 
 namespace locksys {
 
-/** A helper function which solves a chicken-and-egg problem occurring when one
-needs to iterate over trx's locks and perform some actions on them. Iterating
-over this list requires trx->mutex (or exclusive global lock_sys latch), and
-operating on a lock requires lock_sys latches, yet the latching order requires
-lock_sys latches to be taken before trx->mutex.
-One way around it is to use exclusive global lock_sys latch, which heavily
-deteriorates concurrency. Another is to try to reacquire the latches in needed
-order, verifying that the list wasn't modified meanwhile.
-This function performs following steps:
-1. releases trx->mutex,
-2. acquires proper lock_sys shard latch,
-3. reacquires trx->mutex
-4. executes f unless trx's locks list has changed
-Before and after this function following should hold:
-- the shared global lock_sys latch is held
-- the trx->mutex is held
-@param[in]    trx     the trx, locks of which we are interested in
-@param[in]    shard   description of the shard we want to latch
-@param[in]    f       the function to execute when the shard is latched
-@return true if f was called, false if it couldn't be called because trx locks
-        have changed while relatching trx->mutex
-*/
-template <typename S, typename F>
-static bool try_relatch_trx_and_shard_and_do(const trx_t *const trx,
-                                             const S &shard, F &&f) {
-  ut_ad(locksys::owns_shared_global_latch());
-  ut_ad(trx_mutex_own(trx));
-
-  const auto expected_version = trx->lock.trx_locks_version;
-  trx_mutex_exit(trx);
-  DEBUG_SYNC_C("try_relatch_trx_and_shard_and_do_noted_expected_version");
-  locksys::Shard_naked_latch_guard guard{UT_LOCATION_HERE, shard};
-  trx_mutex_enter_first_of_two(trx);
-
-  /* Check that list was not modified while we were reacquiring latches */
-  if (expected_version != trx->lock.trx_locks_version) {
-    /* Someone has modified the list while we were re-acquiring the latches so,
-    it is unsafe to operate on the lock. It might have been released, or maybe
-    even assigned to another transaction (in case of AUTOINC lock). More
-    importantly, we need to let know the caller that the list it is iterating
-    over has been modified, which affects next/prev pointers. */
-    return false;
-  }
-
-  std::forward<F>(f)();
-  return true;
-}
-
 /** A helper function which solves a chicken-and-egg problem occurring when one
 needs to iterate over trx's locks and perform some actions on them. Iterating
 over this list requires trx->mutex (or exclusive global lock_sys latch), and
@@ -4217,14 +4134,26 @@ Before and after this function following should hold:
 */
 template <typename F>
 static bool try_relatch_trx_and_shard_and_do(const lock_t *lock, F &&f) {
-  if (lock_get_type_low(lock) == LOCK_REC) {
-    return try_relatch_trx_and_shard_and_do(lock->trx, lock->rec_lock.page_id,
-                                            std::forward<F>(f));
-  }
+  ut_ad(locksys::owns_shared_global_latch());
+  const trx_t *trx = lock->trx;
+  ut_ad(trx_mutex_own(trx));
 
-  ut_ad(lock_get_type_low(lock) == LOCK_TABLE);
-  return try_relatch_trx_and_shard_and_do(lock->trx, *lock->tab_lock.table,
-                                          std::forward<F>(f));
+  const auto expected_version = trx->lock.trx_locks_version;
+  return latch_peeked_shard_and_do(lock, [&]() {
+    ut_ad(trx_mutex_own(trx));
+    /* Check that list was not modified while we were reacquiring latches */
+    if (expected_version != trx->lock.trx_locks_version) {
+      /* Someone has modified the list while we were re-acquiring the latches
+      so, it is unsafe to operate on the lock. It might have been released, or
+      maybe even assigned to another transaction (in case of AUTOINC lock). More
+      importantly, we need to let know the caller that the list it is iterating
+      over has been modified, which affects next/prev pointers. */
+      return false;
+    }
+    std::forward<F>(f)();
+    ut_ad(trx_mutex_own(trx));
+    return true;
+  });
 }
 
 /** We don't want to hold the Global latch for too long, even in S mode, not to
@@ -6139,40 +6068,16 @@ page_id_t lock_rec_get_page_id(const lock_t *lock) {
   return lock->rec_lock.page_id;
 }
 
-/** Cancels a waiting lock request and releases possible other transactions
-waiting behind it.
-@param[in,out]  lock            Waiting lock request */
-void lock_cancel_waiting_and_release(lock_t *lock) {
-  /* Requiring exclusive global latch serves several purposes here.
-
-  1. In case of table LOCK_TABLE we will call lock_release_autoinc_locks(),
-  which iterates over locks held by this transaction and it is not clear if
-  these locks are from the same table. Frankly it is not clear why we even
-  release all of them here (note that none of them is our `lock` because we
-  don't store waiting locks in the trx->autoinc_locks vector, only granted).
-  Perhaps this is because this trx is going to be rolled back anyway, and this
-  seemed to be a good moment to release them?
-
-  2. During lock_rec_dequeue_from_page() and lock_table_dequeue() we might latch
-  trx mutex of another transaction to grant it a lock. The rules meant to avoid
-  deadlocks between trx mutex require us to either use an exclusive global
-  latch, or to first latch trx which is has trx->lock.wait_lock == nullptr.
-  As `lock == lock->trx->lock.wait_lock` and thus is not nullptr, we have to use
-  the first approach, or complicate the proof of deadlock avoidance enormously.
-  */
-  ut_ad(locksys::owns_exclusive_global_latch());
-  /* We will access lock->trx->lock.autoinc_locks which requires trx->mutex */
-  ut_ad(trx_mutex_own(lock->trx));
+void lock_cancel_waiting_and_release(trx_t *trx) {
+  ut_ad(trx_mutex_own(trx));
+  const auto lock = trx->lock.wait_lock.load();
+  ut_ad(locksys::owns_lock_shard(lock));
 
   if (lock_get_type_low(lock) == LOCK_REC) {
     lock_rec_dequeue_from_page(lock);
   } else {
     ut_ad(lock_get_type_low(lock) & LOCK_TABLE);
 
-    if (lock->trx->lock.autoinc_locks != nullptr) {
-      lock_release_autoinc_locks(lock->trx);
-    }
-
     lock_table_dequeue(lock);
   }
 
@@ -6289,34 +6194,31 @@ void lock_trx_release_locks(trx_t *trx) /*!< in/out: transaction */
   trx_mutex_exit(trx);
 }
 
-/** Check whether the transaction has already been rolled back because it
- was selected as a deadlock victim, or if it has to wait then cancel
- the wait lock.
- @return DB_DEADLOCK, DB_LOCK_WAIT or DB_SUCCESS */
-dberr_t lock_trx_handle_wait(trx_t *trx) /*!< in/out: trx lock state */
-{
-  dberr_t err;
-
-  /* lock_cancel_waiting_and_release() requires exclusive global latch, and so
-  does reading the trx->lock.wait_lock to prevent races with B-tree page
-  reorganization */
-  locksys::Global_exclusive_latch_guard guard{UT_LOCATION_HERE};
-
-  trx_mutex_enter(trx);
-
-  if (trx->lock.was_chosen_as_deadlock_victim) {
-    err = DB_DEADLOCK;
-  } else if (trx->lock.wait_lock != nullptr) {
-    lock_cancel_waiting_and_release(trx->lock.wait_lock);
-    err = DB_LOCK_WAIT;
-  } else {
-    /* The lock was probably granted before we got here. */
-    err = DB_SUCCESS;
-  }
-
-  trx_mutex_exit(trx);
-
-  return (err);
+bool lock_cancel_if_waiting_and_release(const TrxVersion trx_version) {
+  trx_t &trx{*trx_version.m_trx};
+  bool realeased = false;
+  locksys::run_if_waiting(trx_version, [&]() {
+    ut_ad(trx_mutex_own(&trx));
+    ut_a(trx_version.m_version == trx.version.load());
+    if ((trx.in_innodb & TRX_FORCE_ROLLBACK) != 0) {
+      /* A HP transaction wants to wake up and rollback trx by pretending it
+      has been chosen a deadlock victim while waiting for a lock. */
+#ifdef UNIV_DEBUG
+      ib::info(ER_IB_MSG_639, to_string(trx.killed_by).c_str(),
+               ulonglong{trx.id});
+#endif /* UNIV_DEBUG */
+      trx.lock.was_chosen_as_deadlock_victim = true;
+    } else {
+      /* This case is currently used by kill_connection. Canceling the
+      wait and waking up the transaction will have the effect that its
+      thread will continue without the lock acquired, which is unsafe,
+      unless it will notice that it has been interrupted and give up. */
+      ut_ad(trx_is_interrupted(&trx));
+    }
+    lock_cancel_waiting_and_release(&trx);
+    realeased = true;
+  });
+  return realeased;
 }
 
 #ifdef UNIV_DEBUG
diff --git a/storage/innobase/lock/lock0wait.cc b/storage/innobase/lock/lock0wait.cc
index 2b5a9386229..139cca0b6cf 100644
--- a/storage/innobase/lock/lock0wait.cc
+++ b/storage/innobase/lock/lock0wait.cc
@@ -435,7 +435,10 @@ void lock_reset_wait_and_release_thread_if_suspended(lock_t *lock) {
   2. debugging, as reseting blocking_trx makes it easier to spot it was not
      properly set on subsequent waits.
   3. helping lock_make_trx_hit_list() notice that HP trx is no longer waiting
-     for a lock, so it can take a fast path */
+     for a lock, so it can take a fast path
+  Also, lock_wait_check_and_cancel() looks if block_trx become nullptr to figure
+  out if wait_lock become nullptr only temporarily (for B-tree reorg) or
+  permanently (due to lock_reset_wait_and_release_thread_if_suspended()) */
   lock->trx->lock.blocking_trx.store(nullptr);
 
   /* We only release locks for which someone is waiting, and we posses a latch
@@ -457,71 +460,60 @@ void lock_reset_wait_and_release_thread_if_suspended(lock_t *lock) {
     lock_wait_release_thread_if_suspended(thr);
   }
 }
-
+static void lock_wait_try_cancel(trx_t *trx, bool timeout) {
+  ut_a(trx->lock.wait_lock != nullptr);
+  ut_ad(locksys::owns_lock_shard(trx->lock.wait_lock));
+  ut_a(trx->lock.que_state == TRX_QUE_LOCK_WAIT);
+  if (trx_is_high_priority(trx)) {
+    /* We know that wait_lock is non-null and have its shard latches, so we can
+    safely read the blocking_trx and assert its not null. */
+    const trx_t *blocker = trx->lock.blocking_trx.load();
+    ut_ad(blocker != nullptr);
+    /* An HP trx should not give up if the blocker is not HP. */
+    if (!trx_is_high_priority(blocker)) {
+      return;
+    }
+  }
+  ut_ad(trx_mutex_own(trx));
+  if (timeout) {
+    /* Make sure we are not overwriting the DB_DEADLOCK which would be more
+    important to report as it rolls back whole transaction, not just the
+    current query. We set error_state to DB_DEADLOCK only:
+    1) before the transaction reserves a slot. But, we know it's in a slot.
+    2) when wait_lock is already set to nullptr. But, it's not nullptr. */
+    ut_ad(trx->error_state != DB_DEADLOCK);
+    trx->error_state = DB_LOCK_WAIT_TIMEOUT;
+    /* This flag can't be set, as we always call the
+    lock_cancel_waiting_and_release() immediately after setting it, which
+    either prevents the trx from going to sleep or resets the wait_lock, and
+    we've ruled out both of these possibilities. This means that the
+    subsequent call to lock_cancel_waiting_and_release() shouldn't overwrite
+    the error_state we've just set. This isn't a crucial property, but makes
+    reasoning simpler, I hope, hence this assert. */
+    ut_ad(!trx->lock.was_chosen_as_deadlock_victim);
+  }
+  /* Cancel the lock request queued by the transaction and release possible
+  other transactions waiting behind. */
+  lock_cancel_waiting_and_release(trx);
+}
 /** Check if the thread lock wait has timed out. Release its locks if the
  wait has actually timed out. */
 static void lock_wait_check_and_cancel(
     const srv_slot_t *slot) /*!< in: slot reserved by a user
                             thread when the wait started */
 {
-  trx_t *trx;
-
   const auto wait_time = std::chrono::steady_clock::now() - slot->suspend_time;
   /* Timeout exceeded or a wrap-around in system time counter */
   const auto timeout = slot->wait_timeout < std::chrono::seconds{100000000} &&
                        wait_time > slot->wait_timeout;
-  trx = thr_get_trx(slot->thr);
-
-  if (trx_is_interrupted(trx) || timeout) {
-    /* The lock_cancel_waiting_and_release() needs exclusive global latch.
-    Also, we need to latch the shard containing wait_lock to read the field and
-    access the lock itself. */
-    locksys::Global_exclusive_latch_guard guard{UT_LOCATION_HERE};
-
-    trx_mutex_enter(trx);
-    bool should_cancel{false};
-    /* It is possible that the lock has already been granted: in that case do
-    nothing. */
-    if (trx->lock.wait_lock != nullptr) {
-      ut_a(trx->lock.que_state == TRX_QUE_LOCK_WAIT);
-      if (trx_is_high_priority(trx)) {
-        /* We read blocking_trx under Global exclusive latch so it can't change
-        and we know that wait_lock is non-null so there must be a blocker. */
-        const trx_t *blocker = trx->lock.blocking_trx.load();
-        ut_ad(blocker != nullptr);
-        /* An HP trx should not give up if the blocker is not HP. */
-        if (trx_is_high_priority(blocker)) {
-          should_cancel = true;
-        }
-      } else {
-        should_cancel = true;
-      }
-    }
-    if (should_cancel) {
-      if (timeout) {
-        /* Make sure we are not overwriting the DB_DEADLOCK which would be more
-        important to report as it rolls back whole transaction, not just the
-        current query. We set error_state to DB_DEADLOCK only:
-        1) before the transaction reserves a slot. But, we know it's in a slot.
-        2) when wait_lock is already set to nullptr. But, it's not nullptr. */
-        ut_ad(trx->error_state != DB_DEADLOCK);
-        trx->error_state = DB_LOCK_WAIT_TIMEOUT;
-        /* This flag can't be set, as we always call the
-        lock_cancel_waiting_and_release() immediately after setting it, which
-        either prevents the trx from going to sleep or resets the wait_lock, and
-        we've ruled out both of these possibilities. This means that the
-        subsequent call to lock_cancel_waiting_and_release() shouldn't overwrite
-        the error_state we've just set. This isn't a crucial property, but makes
-        reasoning simpler, I hope, hence this assert. */
-        ut_ad(!trx->lock.was_chosen_as_deadlock_victim);
-      }
-      /* Cancel the lock request queued by the transaction and release possible
-      other transactions waiting behind. */
-      lock_cancel_waiting_and_release(trx->lock.wait_lock);
-    }
+  trx_t *trx = thr_get_trx(slot->thr);
 
-    trx_mutex_exit(trx);
+  if (!trx_is_interrupted(trx) && !timeout) {
+    return;
   }
+  /* We don't expect trx to commit (change version) as we hold lock_wait mutex
+  preventing the trx from leaving the slot. */
+  locksys::run_if_waiting({trx}, [&]() { lock_wait_try_cancel(trx, timeout); });
 }
 
 /** A snapshot of information about a single slot which was in use at the moment
@@ -699,16 +691,14 @@ static void lock_wait_build_wait_for_graph(
 @param[in,out]    chosen_victim   the transaction that should be rolled back */
 static void lock_wait_rollback_deadlock_victim(trx_t *chosen_victim) {
   ut_ad(!trx_mutex_own(chosen_victim));
-  /* The call to lock_cancel_waiting_and_release requires exclusive latch on
-  whole lock_sys.
-  Also, we need to latch the shard containing wait_lock to read it and access
+  /* We need to latch the shard containing wait_lock to read it and access
   the lock itself.*/
   ut_ad(locksys::owns_exclusive_global_latch());
   trx_mutex_enter(chosen_victim);
   chosen_victim->lock.was_chosen_as_deadlock_victim = true;
   ut_a(chosen_victim->lock.wait_lock != nullptr);
   ut_a(chosen_victim->lock.que_state == TRX_QUE_LOCK_WAIT);
-  lock_cancel_waiting_and_release(chosen_victim->lock.wait_lock);
+  lock_cancel_waiting_and_release(chosen_victim);
   trx_mutex_exit(chosen_victim);
 }
 
diff --git a/storage/innobase/row/row0sel.cc b/storage/innobase/row/row0sel.cc
index 778d3c03d9f..a19c010f39d 100644
--- a/storage/innobase/row/row0sel.cc
+++ b/storage/innobase/row/row0sel.cc
@@ -5170,9 +5170,15 @@ rec_loop:
         goto normal_return;
       }
     }
-
-    err = sel_set_rec_lock(pcur, rec, index, offsets, prebuilt->select_mode,
-                           prebuilt->select_lock_type, lock_type, thr, &mtr);
+    /* in case of semi-consistent read, we use SELECT_SKIP_LOCKED, so we don't
+    waste time on creating a WAITING lock, as we won't wait on it anyway */
+    const bool use_semi_consistent =
+        prebuilt->row_read_type == ROW_READ_TRY_SEMI_CONSISTENT &&
+        !unique_search && index == clust_index && !trx_is_high_priority(trx);
+    err = sel_set_rec_lock(
+        pcur, rec, index, offsets,
+        use_semi_consistent ? SELECT_SKIP_LOCKED : prebuilt->select_mode,
+        prebuilt->select_lock_type, lock_type, thr, &mtr);
 
     switch (err) {
       const rec_t *old_vers;
@@ -5191,54 +5197,22 @@ rec_loop:
         }
         break;
       case DB_SKIP_LOCKED:
-        goto next_rec;
-      case DB_LOCK_WAIT:
-        /* Lock wait for R-tree should already
-        be handled in sel_set_rtr_rec_lock() */
-        ut_ad(!dict_index_is_spatial(index));
-        /* Never unlock rows that were part of a conflict. */
-        prebuilt->new_rec_lock.reset();
-
-        if (UNIV_LIKELY(prebuilt->row_read_type !=
-                        ROW_READ_TRY_SEMI_CONSISTENT) ||
-            unique_search || index != clust_index) {
-          goto lock_wait_or_error;
+        if (prebuilt->select_mode == SELECT_SKIP_LOCKED) {
+          goto next_rec;
         }
+        DEBUG_SYNC_C("semi_consistent_read_would_wait");
+        ut_a(use_semi_consistent);
         ut_a(trx->allow_semi_consistent());
         /* The following call returns 'offsets' associated with 'old_vers' */
         row_sel_build_committed_vers_for_mysql(
             clust_index, prebuilt, rec, &offsets, &heap, &old_vers,
             need_vrow ? &vrow : nullptr, &mtr);
 
-        /* Check whether it was a deadlock or not, if not
-        a deadlock and the transaction had to wait then
-        release the lock it is waiting on. */
-
-        DEBUG_SYNC_C("semi_consistent_read_would_wait");
-        err = lock_trx_handle_wait(trx);
-
-        switch (err) {
-          case DB_SUCCESS:
-            /* The lock was granted while we were
-            searching for the last committed version.
-            Do a normal locking read. */
-
-            offsets = rec_get_offsets(rec, index, offsets, ULINT_UNDEFINED,
-                                      UT_LOCATION_HERE, &heap);
-            goto locks_ok;
-          case DB_DEADLOCK:
-            goto lock_wait_or_error;
-          case DB_LOCK_WAIT:
-            ut_ad(!dict_index_is_spatial(index));
-            err = DB_SUCCESS;
-            break;
-          default:
-            ut_error;
-        }
+        ut_ad(!dict_index_is_spatial(index));
+        err = DB_SUCCESS;
 
         if (old_vers == nullptr) {
           /* The row was not yet committed */
-
           goto next_rec;
         }
 
@@ -5249,6 +5223,14 @@ rec_loop:
                  pcur, index, prev_rec, &prev_rec_debug_n_fields,
                  &prev_rec_debug_buf, &prev_rec_debug_buf_size));
         break;
+      case DB_LOCK_WAIT:
+        /* Lock wait for R-tree should already
+        be handled in sel_set_rtr_rec_lock() */
+        ut_ad(!dict_index_is_spatial(index));
+        /* Never unlock rows that were part of a conflict. */
+        prebuilt->new_rec_lock.reset();
+        ut_a(!use_semi_consistent);
+        goto lock_wait_or_error;
       case DB_RECORD_NOT_FOUND:
         if (dict_index_is_spatial(index)) {
           goto next_rec;
@@ -5257,10 +5239,9 @@ rec_loop:
         }
 
       default:
-
+        ut_a(!use_semi_consistent);
         goto lock_wait_or_error;
     }
-  locks_ok:
     if (err == DB_SUCCESS && !row_to_range_relation.row_can_be_in_range) {
       err = DB_RECORD_NOT_FOUND;
       goto normal_return;
diff --git a/storage/innobase/trx/trx0trx.cc b/storage/innobase/trx/trx0trx.cc
index 5610541255f..fb8b98cf9ae 100644
--- a/storage/innobase/trx/trx0trx.cc
+++ b/storage/innobase/trx/trx0trx.cc
@@ -2706,8 +2706,11 @@ Assumption 1.
   If a thread attempts to acquire more then one trx->mutex, then it either has
   exclusive global latch, or it attempts to acquire exactly two of them, and at
   just before calling mutex_enter for the second time it saw
-  trx1->lock.wait_lock==nullptr, trx2->lock.wait_lock!=nullptr, and it held the
-  latch for the shard containing trx2->lock.wait_lock.
+  1.1 trx1->lock.wait_lock==nullptr or it held the latch for the shard
+      containing the trx1->lock.wait_lock
+  AND
+  1.2. trx2->lock.wait_lock!=nullptr, and it held the latch for the shard
+       containing trx2->lock.wait_lock.
 
 @see asserts in trx_before_mutex_enter
 
@@ -2743,9 +2746,14 @@ By proving the theorem, and observing that the assertions hold for multiple runs
 of test suite on debug build, we gain more and more confidence that
 trx_mutex_enter() calls can not deadlock.
 
-The intuitive, albeit imprecise, version of the proof is that by Assumption 1
-each edge of the deadlock cycle leads from a trx with NULL trx->lock.wait_lock
-to one with non-NULL wait_lock, which means it has only one edge.
+The intuitive, albeit imprecise, version of the proof is that by Assumption 1,
+for each i, the edge of the deadlock cycle caused by thread[i] - which has
+already latched transaction start[i] and waits to latch transaction end[i] -
+must have non-NULL end[i]->lock.wait_lock in a shard latched by thread[i], and
+either start[i]->lock.wait_lock == NULL, or the shard containing this lock is
+latched by thread[i], in either case no other j!=i can have end[j]=start[i], as
+that would mean start[i]->lock.wait_lock is non-NULL and in shard latched by
+thread[j] instead.
 
 The difficulty lays in that wait_lock is a field which can be modified over time
 from several threads, so care must be taken to clarify at which moment in time
@@ -2767,9 +2775,22 @@ Fact 4. Another thread has latched trx_a->mutex as the first of its two latches
 Consider the situation from the point of view of this other thread, which is now
 in the deadlock waiting for mutex_enter(trx_b->mutex) for some trx_b!=trx_a.
 By Fact 2 and assumption 1, it had to take the "else" branch on the way there,
-and thus it has saw: trx_a->lock.wait_lock == nullptr at some moment in time.
+and thus at some moment in time it has saw either
+a) trx_a->lock.wait_lock == nullptr or
+b) it held the latch of the shard containing trx_a->lock.wait_lock.
 This observation was either before or after our observation that
-trx_a->lock.wait_lock != nullptr (again Fact 2 and Assumption 1).
+trx_a->lock.wait_lock != nullptr and that we hold the latch on this shard
+(again Fact 2 and Assumption 1).
+
+Let us first rule out case b). As both threads are presumably in a deadlock,
+they are still holding the latches on the lock-sys shards that they had latched,
+so in case b) both threads hold a latch on a shard which contained the current
+trx_a->lock.wait_lock value, which implies they must have latched two different
+shards, which implies they saw two different values of wait_lock field, which in
+turn means that it has changed, but by Assumption 4 it can not change while
+somebody holds a latch on its current value's shard. Thus b) is impossible!
+
+This leaves us with case a).
 
 If our thread observed non-NULL value first, then it means a change from
 non-NULL to NULL has happened, which by Assumption 4 requires a shard latch,
@@ -2802,7 +2823,11 @@ void trx_before_mutex_enter(const trx_t *trx, bool first_of_two) {
     if (!locksys::owns_exclusive_global_latch()) {
       ut_a(trx_allowed_two_latches);
       ut_a(trx_latched_count == 2);
-      ut_a(trx_first_latched_trx->lock.wait_lock == nullptr);
+      /* In theory wait_lock can change from non-null to null at any moment
+      unless we indeed hold the wait_lock's shard. Thankfully, this is exactly
+      what we assert. */
+      ut_a(trx_first_latched_trx->lock.wait_lock == nullptr ||
+           locksys::owns_lock_shard(trx_first_latched_trx->lock.wait_lock));
       ut_a(trx_first_latched_trx != trx);
       /* This is not very safe, because to read trx->lock.wait_lock we
       should already either latch trx->mutex (which we don't) or shard with
@@ -3433,12 +3458,16 @@ void trx_kill_blocking(trx_t *trx) {
 
   for (hit_list_t::reverse_iterator it = hit_list.rbegin(); it != end; ++it) {
     trx_t *victim_trx = it->m_trx;
-    ulint version = it->m_version;
+    auto version = it->m_version;
 
     /* Shouldn't commit suicide. */
     ut_ad(victim_trx != trx);
     ut_ad(victim_trx->mysql_thd != trx->mysql_thd);
 
+    if (lock_cancel_if_waiting_and_release(*it)) {
+      continue;
+    }
+
     /* Check that the transaction isn't active inside
     InnoDB code. We have to wait while it is executing
     in the InnoDB context. This can potentially take a
@@ -3524,10 +3553,9 @@ void trx_kill_blocking(trx_t *trx) {
     trx_rollback_for_mysql(victim_trx);
 
 #ifdef UNIV_DEBUG
-    ib::info(ER_IB_MSG_1211)
-        << "High Priority Transaction (ID): " << trx->id
-        << " killed transaction (ID): " << id << " in hit list"
-        << " - " << thr_text;
+    ib::info(ER_IB_MSG_1211, ulonglong{trx->id},
+             to_string(std::this_thread::get_id()).c_str(), ulonglong{id},
+             thr_text);
 #endif /* UNIV_DEBUG */
     trx_mutex_enter(victim_trx);
 
